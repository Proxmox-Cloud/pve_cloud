# for kubernetes clusters the node creation and kubespray bootstrap are done in one go
# in the pve.cloud.sync_kubespray playbook
target_pve: pve-cluster-name.pve-cloud-domain.example
stack_name: sample-cluster

# these entries will populate the dedicated cluster tls certificate
# not to be confused with the kubeapi cert. this is a cert meant to be
# used by the ingress controller
cluster_cert_entries:
  - zone: example.com
    authoritative_zone: true
    names: 
      - '*.subdomain'
      - '*'
  - zone: another.sample
    names: 
      - '*'

# domains and names listed here will bind to the external haproxy floating ip
external_domains:
  - zone: example.com
    names:
      - my-public-service
  - zone: another.sample
    names:
      - '*' # all names get exposed

# our cluster haproxy can also forward special tcp traffic to nodeport nodes
tcp_proxies:
  - proxy_name: postgres-sample
    haproxy_port: 5342
    node_port: 30342 # nodeport its exposed under
    external: true

# these default pve cloud stacks are needed for bootstrapping the cluster
# dhcp will automatically make static reservations for the node ips initially assigned
# via dhcp. bind is used for masters recordset with all masters ips
# postgres for state records and proxy for reconfiguring and reloading
static_includes:
  # stack names of the other inventories
  dhcp_stack: dhcp.pve-cloud-domain.example
  proxy_stack: haproxy.pve-cloud-domain.example
  postgres_stack: patroni.pve-cloud-domain.example
  bind_stack: bind.pve-cloud-domain.example

# our nodes we want to create, they will also be kept in sync everytime you execute the playbook
# if you need a unique node with labels/taints you have to assign a unique hostname
qemus:
- k8s_roles: ['master']
  disk:
    size: 50G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 2
    memory: 4096
- k8s_roles: ['master']
  disk:
    size: 50G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 2
    memory: 4096
- k8s_roles: ['master']
  disk:
    size: 50G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 2
    memory: 4096
# workers
- k8s_roles: ['worker']
  disk:
    size: 175G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 4
    memory: 16384
- hostname: example-service-exclusive
  k8s_roles: ['worker']
  disk:
    size: 175G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 4
    memory: 16384
  vars:
    # any label / taint you want to toggle on
    node_labels:
      exclusive-service: example
    node_taints:
      - exclusive-service=example:NoSchedule

# these pools will be made available as storage classes inside k8s
# the ceph csi storage provisioner chart is also installed automatically by
# the playbook
ceph_csi_sc_pools:
  - name: ssd
    default: true
    mount_options:
      - discard

root_ssh_pub_key: 'ssh-ed25519 AAAeAC3NzaC1tlZDI1NdasdAAIGreJ6EeeGddasB30hv1h5+erHhreUHzvmYE' # your public ssh key

# base parameters all nodes share
qemu_base_parameters:
  # this config assumes your ceph is on the same interface as default vm communication
  # setup an additional bridge with ceph if you have a seperate net
  net0: virtio,bridge=vmbr0,firewall=1,tag=100
  onboot: 1

acme_staging: false # toggle prod generation for acme certs, otherwise goes against letsencrypt staging

plugin: pve.cloud.kubespray_inv
