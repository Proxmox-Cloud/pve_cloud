# for kubernetes clusters the node creation and kubespray bootstrap are done in one go
# in the pve.cloud.sync_kubespray playbook
target_pve: pve-cluster-name.pve-cloud-domain.example
stack_name: sample-cluster

# these entries will populate the dedicated cluster tls certificate
# not to be confused with the kubeapi cert. this is a cert meant to be
# used by the ingress controller, it will only be generated if you defined an
# acme_contact and acme_method in cloud-inv.yaml
cluster_cert_entries:
  - zone: example.com
    # will make bind authoritative for this zone
    # meaning it will create the zone in bind and assume it holds all
    # record for that zone. this is needed for k8s dynamic ingress dns
    authoritative_zone: true 
    names: 
      - '*.subdomain'
      - '*'
  - zone: another.sample
    names: 
      - '*'

# domains and names listed here will bind to the external haproxy floating ip
external_domains:
  - zone: example.com
    names:
      - my-public-service
  - zone: another.sample
    names:
      - '*' # all names get exposed

# our cluster haproxy can also forward special tcp traffic to nodeport nodes
tcp_proxies:
  - proxy_name: postgres-sample
    haproxy_port: 5342
    node_port: 30342 # nodeport its exposed under
    external: true

# these default pve cloud stacks are needed for bootstrapping the cluster
# dhcp will automatically make static reservations for the node ips initially assigned
# via dhcp. bind is used for masters recordset with all masters ips
# postgres for state records and proxy for reconfiguring and reloading
static_includes:
  # stack names of the other inventories
  dhcp_stack: dhcp.pve-cloud-domain.example
  proxy_stack: haproxy.pve-cloud-domain.example
  postgres_stack: patroni.pve-cloud-domain.example
  bind_stack: bind.pve-cloud-domain.example

# our nodes we want to create, they will also be kept in sync everytime you execute the playbook
# if you need a unique node with labels/taints you have to assign a unique hostname
qemus:
# the number of master nodes should be uneven - 1,3,5
- k8s_roles: ['master']
  disk:
    size: 50G
    # mount option for ssd backed storage
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 2
    memory: 4096 # 3 gigs is min, 4gb is recommended

# workers
- k8s_roles: ['worker']
  disk:
    size: 125G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 4
    memory: 8192
- hostname: example-service-exclusive
  k8s_roles: ['worker']
  disk:
    size: 100G
    options:
      discard: "on"
      iothread: "on"
      ssd: "on"
    pool: ssd
  parameters:
    sockets: 1
    cores: 4
    memory: 8192
  vars:
    # any label / taint you want to toggle on
    node_labels:
      exclusive-service: example
    node_taints:
      - exclusive-service=example:NoSchedule

root_ssh_pub_key: 'ssh-ed25519 AAAeAC3NzaC1tlZDI1NdasdAAIGreJ6EeeGddasB30hv1h5+erHhreUHzvmYE'

# base parameters all nodes share
qemu_base_parameters:
  # this config assumes your ceph is on the same interface as default vm communication
  # setup an additional bridge with ceph if you have a seperate net
  net0: virtio,bridge=vmbr0,firewall=1,tag=100
  onboot: 1

acme_staging: false # toggle prod generation for acme certs, otherwise goes against letsencrypt staging

plugin: pve.cloud.kubespray_inv
