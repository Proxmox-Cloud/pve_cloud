global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
        stats timeout 30s
        user haproxy
        group haproxy
        daemon

        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private

        # See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets


# we need this for ddns, hosts might not instantly have dns records
# since they only get one after dhclient sends a request
resolvers pve-cloud-dns
        nameserver dns1 {{ bind_master_ip }}:53
        nameserver dns2 {{ bind_slave_ip }}:53
        resolve_retries       3
        timeout retry         1s
        hold valid            10s


defaults
        log     global
        mode    http
        option  httplog
        option  dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http
        # added this to allow startup if even if not all backends can be resolved
        # also add default pve cloud resolver
        default-server init-addr last,libc,none resolvers pve-cloud-dns


frontend haproxy-statistics
        bind {{ pve_haproxy_floating_ip_internal }}:1936
        mode http
        stats uri /stats
        stats show-legends
        option dontlog-normal


# for pve cloud monitoring rules
frontend prometheus
        bind {{ pve_haproxy_floating_ip_internal }}:8405
        mode http
        http-request use-service prometheus-exporter
        no log


# access to ha postgres (state store/orm for pve cloud)
listen ha-postgres
        mode tcp
        bind {{ pve_haproxy_floating_ip_internal }}:5000
        
        # logic for selecting current master
        option httpchk
        http-check expect status 200
        default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions

{% for host in groups['postgres_stack'] %}
        server {{ hostvars[host].ansible_hostname }} {{ hostvars[host].ansible_hostname }}.{{ pve_cloud_domain }}:5432 maxconn 100 check port 8008
{% endfor %}


# generic https ingress for k8s clusters
frontend k8s-ingress-internal
        mode tcp
        bind {{ pve_haproxy_floating_ip_internal }}:443

        # Extract SNI (Server Name Indication)
        tcp-request inspect-delay 5s
        tcp-request content accept if { req_ssl_hello_type 1 }

        # internal get all ingress rules
{% for ingress_rule in k8s_ingress_rules.query_result | sort(attribute='rule_len', reverse=true) %}
{% if ingress_rule.is_k8s %}
        use_backend k8s-ingress_{{ ingress_rule.stack_fqdn }} if { req.ssl_sni -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% else %}
        use_backend vms-ingress_{{ ingress_rule.stack_fqdn }} if { req.ssl_sni -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% endif %}
{% endfor %}



# specific external floating ip for access control
frontend k8s-ingress-external
        mode tcp
        bind {{ pve_haproxy_floating_ip_external }}:443

        # Extract SNI (Server Name Indication)
        tcp-request inspect-delay 5s
        tcp-request content accept if { req_ssl_hello_type 1 }

        # external gets only rules specifically declared for external
{% for ingress_rule in k8s_ingress_rules.query_result | sort(attribute='rule_len', reverse=true) if ingress_rule.external %}
{% if ingress_rule.is_k8s %}
        use_backend k8s-ingress_{{ ingress_rule.stack_fqdn }} if { req.ssl_sni -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% else %}
        use_backend vms-ingress_{{ ingress_rule.stack_fqdn }} if { req.ssl_sni -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% endif %}

{% endfor %}


# create https backend for all cluster stacks
{% for stack_fqdn, workers in k8s_workers_by_stack.items() %}
backend k8s-ingress_{{ stack_fqdn }}
        mode tcp
{% for worker in workers %}
        server {{ worker.hostname }} {{ worker.ip }}:443 check send-proxy-v2
{% endfor %}


{% endfor %}

# create https backend for vms / lxc that have ingress_domains defined
{% for stack_fqdn, machines in ingress_vms_by_stack.items() %}
backend vms-ingress_{{ stack_fqdn }}
        mode tcp
{% for machine in machines %}
        server {{ machine.hostname }} {{ machine.ip }}:443 check
{% endfor %}


{% endfor %}


frontend k8s-ingress-http-internal
        mode tcp
        bind {{ pve_haproxy_floating_ip_internal }}:80

        # internal get all ingress rules
{% for ingress_rule in k8s_ingress_rules.query_result | sort(attribute='rule_len', reverse=true)  %}
{% if ingress_rule.is_k8s %}
        use_backend k8s-ingress-http_{{ ingress_rule.stack_fqdn }} if { hdr(host) -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% else %}
        use_backend vms-ingress-http_{{ ingress_rule.stack_fqdn }} if { hdr(host) -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% endif %}

{% endfor %}


frontend k8s-ingress-http-external
        mode tcp
        bind {{ pve_haproxy_floating_ip_external }}:80

        # only external marked rules
{% for ingress_rule in k8s_ingress_rules.query_result | sort(attribute='rule_len', reverse=true) if ingress_rule.external %}
{% if ingress_rule.is_k8s %}
        use_backend k8s-ingress-http_{{ ingress_rule.stack_fqdn }} if { hdr(host) -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% else %}
        use_backend vms-ingress-http_{{ ingress_rule.stack_fqdn }} if { hdr(host) -m reg ^{{ ingress_rule.name | regex_replace('\.', '\\.') | regex_replace('\\*', '.*')  }}\.{{ ingress_rule.zone | regex_replace('\.', '\\.') }}$ }
{% endif %}

{% endfor %}




# create backend for all cluster stacks
{% for stack_fqdn, workers in k8s_workers_by_stack.items() %}
backend k8s-ingress-http_{{ stack_fqdn }}
        mode tcp
{% for worker in workers %}
        server {{ worker.hostname }} {{ worker.ip }}:80 check send-proxy-v2
{% endfor %}


{% endfor %}

# create backend for vms / lxcs that have ingress_domains defined
{% for stack_fqdn, machines in ingress_vms_by_stack.items() %}
backend vms-ingress-http_{{ stack_fqdn }}
        mode tcp
{% for machine in machines %}
        server {{ machine.hostname }} {{ machine.ip }}:80 check
{% endfor %}


{% endfor %}



# we also expose the controlplane via our proxy to provide ha
frontend k8s-control-plane-internal
        mode tcp
        bind {{ pve_haproxy_floating_ip_internal }}:6443

        # Extract SNI (Server Name Indication)
        tcp-request inspect-delay 5s
        tcp-request content accept if { req_ssl_hello_type 1 }

{% for stack_fqdn in k8s_masters_by_stack.keys() %}     
        use_backend k8s-control-plane_{{ stack_fqdn }} if { req.ssl_sni -m reg ^control-plane-{{ stack_fqdn }}$ }
{% endfor %}
{% for control_plane in k8s_ext_control_planes.query_result %}
{% for extra_san in control_plane.extra_sans.split(',') %}
        use_backend k8s-control-plane_{{ control_plane.stack_fqdn }} if { req.ssl_sni -m reg ^{{ extra_san | regex_replace('\.', '\\.') }}$ }
{% endfor %}
{% endfor %}


# sometimes we might want to expose the controlplane to the world wide web
frontend k8s-control-plane-external
        mode tcp
        bind {{ pve_haproxy_floating_ip_external }}:6443

        # Extract SNI (Server Name Indication)
        tcp-request inspect-delay 5s
        tcp-request content accept if { req_ssl_hello_type 1 }

{% for control_plane in k8s_ext_control_planes.query_result %}
{% for extra_san in control_plane.extra_sans.split(',') %}
        use_backend k8s-control-plane_{{ control_plane.stack_fqdn }} if { req.ssl_sni -m reg ^{{ extra_san | regex_replace('\.', '\\.') }}$ }
{% endfor %}
{% endfor %}


# create backend for all cluster stacks
{% for stack_fqdn, masters in k8s_masters_by_stack.items() %}
backend k8s-control-plane_{{ stack_fqdn }}
        mode tcp
{% for master in masters %}
        server {{ master.hostname }} {{ master.ip }}:6443 check
{% endfor %}

{% endfor %}


# internal tcp proxies to k8s workers
{% for tcp_proxy in k8s_tcp_proxies.query_result %}
listen k8s-tcp-frontend-internal_{{ tcp_proxy.stack_fqdn }}_{{ tcp_proxy.proxy_name }}
        mode tcp
        bind {{ pve_haproxy_floating_ip_internal }}:{{ tcp_proxy.haproxy_port }}

{% if tcp_proxy.proxy_snippet %}
        {{ tcp_proxy.proxy_snippet | indent(8) }}
{% endif %}

{% if tcp_proxy.is_k8s %}
{% for machine in kea_reservations_by_stack[tcp_proxy.stack_fqdn]['k8s_worker'] %}
        server {{ machine.hostname }} {{ machine.ip }}:{{ tcp_proxy.node_port }} check
{% endfor %}
{% else %}

# todo: this is hacky and should be redone
{% if 'qemu' in kea_reservations_by_stack[tcp_proxy.stack_fqdn] %}
{% for machine in kea_reservations_by_stack[tcp_proxy.stack_fqdn]['qemu'] %}
        server {{ machine.hostname }} {{ machine.ip }}:{{ tcp_proxy.node_port }} check
{% endfor %} 
{% endif %}

{% if 'lxc' in kea_reservations_by_stack[tcp_proxy.stack_fqdn] %}
{% for machine in kea_reservations_by_stack[tcp_proxy.stack_fqdn]['lxc'] %}
        server {{ machine.hostname }} {{ machine.ip }}:{{ tcp_proxy.node_port }} check
{% endfor %} 
{% endif %}


{% endif %}


{% endfor %}

# external tcp proxies to k8s workers
{% for tcp_proxy in k8s_tcp_proxies.query_result if tcp_proxy.external %}
listen k8s-tcp-frontend-external_{{ tcp_proxy.stack_fqdn }}_{{ tcp_proxy.proxy_name }}
        mode tcp
        bind {{ pve_haproxy_floating_ip_external }}:{{ tcp_proxy.haproxy_port }}

{% if tcp_proxy.proxy_snippet %}
        {{ tcp_proxy.proxy_snippet | indent(8) }}
{% endif %}


{% if tcp_proxy.is_k8s %}
{% for machine in kea_reservations_by_stack[tcp_proxy.stack_fqdn]['k8s_worker'] %}
        server {{ machine.hostname }} {{ machine.ip }}:{{ tcp_proxy.node_port }} check
{% endfor %}
{% else %}

# todo: this is hacky and should be redone
{% if 'qemu' in kea_reservations_by_stack[tcp_proxy.stack_fqdn] %}
{% for machine in kea_reservations_by_stack[tcp_proxy.stack_fqdn]['qemu'] %}
        server {{ machine.hostname }} {{ machine.ip }}:{{ tcp_proxy.node_port }} check
{% endfor %} 
{% endif %}

{% if 'lxc' in kea_reservations_by_stack[tcp_proxy.stack_fqdn] %}
{% for machine in kea_reservations_by_stack[tcp_proxy.stack_fqdn]['lxc'] %}
        server {{ machine.hostname }} {{ machine.ip }}:{{ tcp_proxy.node_port }} check
{% endfor %} 
{% endif %}


{% endif %}


{% endfor %}
